name: MCP Test Harness - Performance Tracking

on:
  schedule:
    # Run performance tracking every 6 hours
    - cron: '0 */6 * * *'
  push:
    branches: [ main ]
    paths:
      - 'crates/codeprism-core/**'
      - 'crates/codeprism-mcp/**'
      - 'crates/codeprism-analysis/**'
  workflow_dispatch:
    inputs:
      benchmark_duration:
        description: 'Benchmark duration (quick/standard/extended)'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - extended

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Performance baseline tracking
  performance-tracking:
    name: Performance Baseline Tracking
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 100  # Need history for trend analysis

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          key: performance-tracking-${{ runner.os }}

      - name: Install performance monitoring tools
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            python3 python3-pip jq time hyperfine \
            linux-perf perf-tools-unstable
          
          pip3 install matplotlib pandas numpy scipy seaborn

      - name: Download historical performance data
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: performance-history
          path: performance-history/

      - name: Build optimized binaries
        run: |
          # Build with consistent optimization settings
          RUSTFLAGS="-C target-cpu=native -C opt-level=3" \
            cargo build --release --all-features
          
          RUSTFLAGS="-C target-cpu=native -C opt-level=3" \
            cargo build --release --benches

      - name: Execute performance benchmarks
        id: benchmarks
        run: |
          # Determine benchmark duration
          case "${{ github.event.inputs.benchmark_duration || 'standard' }}" in
            "quick")
              BENCHMARK_TIME="--quick"
              WARMUP_TIME="1s"
              MEASURE_TIME="3s"
              ;;
            "extended")
              BENCHMARK_TIME=""
              WARMUP_TIME="5s"
              MEASURE_TIME="30s"
              ;;
            *)
              BENCHMARK_TIME=""
              WARMUP_TIME="3s"
              MEASURE_TIME="10s"
              ;;
          esac
          
          echo "Running benchmarks with warmup: $WARMUP_TIME, measure: $MEASURE_TIME"
          
          # Core performance benchmarks
          cargo bench --bench comprehensive $BENCHMARK_TIME -- \
            --output-format json --warm-up-time $WARMUP_TIME --measurement-time $MEASURE_TIME \
            > current-performance.json
          
          # MCP tool-specific benchmarks
          cd crates/codeprism-mcp
          cargo bench $BENCHMARK_TIME -- \
            --output-format json --warm-up-time $WARMUP_TIME --measurement-time $MEASURE_TIME \
            > ../../mcp-performance.json
          cd ../..
          
          # Test harness performance
          cd crates/codeprism-test-harness
          time cargo run --release -- \
            --config config/core-tools/repository_stats/python.yaml \
            --performance-benchmark --iterations 10 \
            > ../../test-harness-performance.txt 2>&1
          cd ../..

      - name: CodePrism Tool Benchmarking
        id: codeprism-benchmarks
        run: |
          echo "Running CodePrism comprehensive tool benchmarking..."
          
          # Build mandrel-mcp-th optimized for benchmarking
          RUSTFLAGS="-C target-cpu=native -C opt-level=3" \
            cargo build --release --package mandrel-mcp-th --bin moth
          
          # Create benchmarking directory
          mkdir -p codeprism-benchmarking
          cd codeprism-benchmarking
          
          # Run comprehensive benchmarks for each language
          for language in rust python java javascript; do
            echo "Benchmarking CodePrism $language comprehensive..."
            
            # Determine iterations based on benchmark duration
            case "${{ github.event.inputs.benchmark_duration || 'standard' }}" in
              "quick")
                ITERATIONS=3
                ;;
              "extended")
                ITERATIONS=15
                ;;
              *)
                ITERATIONS=10
                ;;
            esac
            
            SPEC_FILE="../crates/codeprism-moth-specs/codeprism/comprehensive/codeprism-$language-comprehensive.yaml"
            
            if [ -f "$SPEC_FILE" ]; then
              echo "  Running $ITERATIONS iterations for $language..."
              
              # Run multiple iterations for statistical significance
              for i in $(seq 1 $ITERATIONS); do
                echo "    Iteration $i/$ITERATIONS"
                
                timeout 180s ../target/release/moth run \
                  "$SPEC_FILE" \
                  --format json \
                  --performance-monitoring \
                  --output "benchmark-$language-iteration-$i.json" \
                  --benchmark-mode || echo "Iteration $i failed or timed out"
              done
              
              # Aggregate results for this language
              python3 - << PYTHON
          import json
          import glob
          import statistics
          from datetime import datetime
          
          language = "$language"
          results = []
          
          # Load all iteration results
          for result_file in glob.glob(f"benchmark-{language}-iteration-*.json"):
              try:
                  with open(result_file, 'r') as f:
                      data = json.load(f)
                  results.append(data)
              except Exception as e:
                  print(f"Could not load {result_file}: {e}")
          
          if results:
              # Calculate aggregate statistics
              total_tests = results[0].get('total_tests', 0)
              
              # Aggregate execution times
              total_durations = []
              for result in results:
                  duration = result.get('total_duration', {})
                  duration_ms = duration.get('secs', 0) * 1000 + duration.get('nanos', 0) / 1e6
                  total_durations.append(duration_ms)
              
              # Individual test performance aggregation
              test_performance = {}
              for result in results:
                  for test in result.get('test_results', []):
                      test_name = test.get('test_name', '')
                      duration = test.get('duration', {})
                      duration_ms = duration.get('secs', 0) * 1000 + duration.get('nanos', 0) / 1e6
                      
                      if test_name not in test_performance:
                          test_performance[test_name] = []
                      test_performance[test_name].append(duration_ms)
              
              # Generate benchmark report
              benchmark_report = {
                  'language': language,
                  'timestamp': datetime.now().isoformat(),
                  'iterations': len(results),
                  'total_tests': total_tests,
                  'suite_performance': {
                      'mean_duration_ms': statistics.mean(total_durations),
                      'median_duration_ms': statistics.median(total_durations),
                      'std_duration_ms': statistics.stdev(total_durations) if len(total_durations) > 1 else 0,
                      'min_duration_ms': min(total_durations),
                      'max_duration_ms': max(total_durations)
                  },
                  'individual_test_performance': {}
              }
              
              # Process individual test statistics
              for test_name, durations in test_performance.items():
                  if durations:
                      benchmark_report['individual_test_performance'][test_name] = {
                          'mean_duration_ms': statistics.mean(durations),
                          'median_duration_ms': statistics.median(durations),
                          'std_duration_ms': statistics.stdev(durations) if len(durations) > 1 else 0,
                          'min_duration_ms': min(durations),
                          'max_duration_ms': max(durations),
                          'samples': len(durations)
                      }
              
              # Save benchmark report
              with open(f'codeprism-benchmark-{language}.json', 'w') as f:
                  json.dump(benchmark_report, f, indent=2)
              
              print(f"‚úÖ CodePrism {language} benchmark completed: {len(results)} iterations")
              print(f"   Average suite duration: {benchmark_report['suite_performance']['mean_duration_ms']:.1f}ms")
              print(f"   Individual tests: {len(benchmark_report['individual_test_performance'])}")
          else:
              print(f"‚ùå No valid results for {language}")
          PYTHON
              
            else
              echo "  ‚ö†Ô∏è  Specification not found: $SPEC_FILE"
            fi
          done
          
          # Move back to root
          cd ..
          
          echo "‚úÖ CodePrism benchmarking completed"

      - name: Analyze performance trends
        id: trend-analysis
        run: |
          python3 - << 'PYTHON'
          import json
          import pandas as pd
          import numpy as np
          import matplotlib.pyplot as plt
          from datetime import datetime, timedelta
          import os
          import glob
          
          # Load current benchmark results
          performance_data = []
          
          # Load core benchmarks
          try:
              with open('current-performance.json', 'r') as f:
                  core_results = json.load(f)
              
              for result in core_results.get('results', []):
                  performance_data.append({
                      'timestamp': datetime.now().isoformat(),
                      'category': 'core',
                      'test_name': result.get('id', ''),
                      'time_ns': result.get('typical', {}).get('estimate', 0),
                      'lower_bound': result.get('typical', {}).get('lower_bound', 0),
                      'upper_bound': result.get('typical', {}).get('upper_bound', 0)
                  })
          except Exception as e:
              print(f"Could not load core benchmarks: {e}")
          
          # Load MCP benchmarks
          try:
              with open('mcp-performance.json', 'r') as f:
                  mcp_results = json.load(f)
              
              for result in mcp_results.get('results', []):
                  performance_data.append({
                      'timestamp': datetime.now().isoformat(),
                      'category': 'mcp',
                      'test_name': result.get('id', ''),
                      'time_ns': result.get('typical', {}).get('estimate', 0),
                      'lower_bound': result.get('typical', {}).get('lower_bound', 0),
                      'upper_bound': result.get('typical', {}).get('upper_bound', 0)
                  })
          except Exception as e:
              print(f"Could not load MCP benchmarks: {e}")
          
          # Load CodePrism benchmarks
          try:
              for language in ['rust', 'python', 'java', 'javascript']:
                  benchmark_file = f'codeprism-benchmarking/codeprism-benchmark-{language}.json'
                  if os.path.exists(benchmark_file):
                      with open(benchmark_file, 'r') as f:
                          codeprism_results = json.load(f)
                      
                      # Add suite-level performance data
                      suite_perf = codeprism_results.get('suite_performance', {})
                      if suite_perf:
                          performance_data.append({
                              'timestamp': datetime.now().isoformat(),
                              'category': 'codeprism_suite',
                              'test_name': f'codeprism_{language}_comprehensive_suite',
                              'time_ns': suite_perf.get('mean_duration_ms', 0) * 1e6,  # Convert ms to ns
                              'lower_bound': suite_perf.get('min_duration_ms', 0) * 1e6,
                              'upper_bound': suite_perf.get('max_duration_ms', 0) * 1e6
                          })
                      
                      # Add individual tool performance data
                      individual_perf = codeprism_results.get('individual_test_performance', {})
                      for tool_name, tool_perf in individual_perf.items():
                          performance_data.append({
                              'timestamp': datetime.now().isoformat(),
                              'category': 'codeprism_tool',
                              'test_name': f'codeprism_{language}_{tool_name}',
                              'time_ns': tool_perf.get('mean_duration_ms', 0) * 1e6,  # Convert ms to ns
                              'lower_bound': tool_perf.get('min_duration_ms', 0) * 1e6,
                              'upper_bound': tool_perf.get('max_duration_ms', 0) * 1e6
                          })
          except Exception as e:
              print(f"Could not load CodePrism benchmarks: {e}")
          
          # Load historical data if available
          historical_data = []
          if os.path.exists('performance-history'):
              for history_file in glob.glob('performance-history/*.json'):
                  try:
                      with open(history_file, 'r') as f:
                          historical_data.extend(json.load(f))
                  except Exception as e:
                      print(f"Could not load {history_file}: {e}")
          
          # Combine historical and current data
          all_data = historical_data + performance_data
          
          # Regression detection
          regressions = []
          improvements = []
          
          if historical_data:
              # Group by test name for trend analysis
              df = pd.DataFrame(all_data)
              df['timestamp'] = pd.to_datetime(df['timestamp'])
              
              for test_name in df['test_name'].unique():
                  test_data = df[df['test_name'] == test_name].sort_values('timestamp')
                  
                  if len(test_data) >= 2:
                      recent_mean = test_data.tail(3)['time_ns'].mean()
                      historical_mean = test_data.head(-1)['time_ns'].mean() if len(test_data) > 3 else test_data.iloc[0]['time_ns']
                      
                      if recent_mean > historical_mean * 1.2:  # 20% regression
                          regressions.append({
                              'test': test_name,
                              'regression_percent': ((recent_mean / historical_mean) - 1) * 100,
                              'historical_mean': historical_mean,
                              'recent_mean': recent_mean
                          })
                      elif recent_mean < historical_mean * 0.9:  # 10% improvement
                          improvements.append({
                              'test': test_name,
                              'improvement_percent': (1 - (recent_mean / historical_mean)) * 100,
                              'historical_mean': historical_mean,
                              'recent_mean': recent_mean
                          })
          
          # Generate performance report
          with open('performance-trend-report.md', 'w') as f:
              f.write("# Performance Trend Analysis\n\n")
              f.write(f"**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n")
              f.write(f"**Commit:** {os.environ.get('GITHUB_SHA', 'unknown')}\n\n")
              
              # Current performance summary
              f.write("## Current Performance Summary\n\n")
              if performance_data:
                  core_tests = [p for p in performance_data if p['category'] == 'core']
                  mcp_tests = [p for p in performance_data if p['category'] == 'mcp']
                  codeprism_suite_tests = [p for p in performance_data if p['category'] == 'codeprism_suite']
                  codeprism_tool_tests = [p for p in performance_data if p['category'] == 'codeprism_tool']
                  
                  f.write(f"- **Core Benchmarks:** {len(core_tests)} tests\n")
                  f.write(f"- **MCP Benchmarks:** {len(mcp_tests)} tests\n")
                  f.write(f"- **CodePrism Suite Benchmarks:** {len(codeprism_suite_tests)} languages\n")
                  f.write(f"- **CodePrism Tool Benchmarks:** {len(codeprism_tool_tests)} tools\n")
                  
                  if core_tests:
                      avg_core_time = np.mean([p['time_ns'] for p in core_tests])
                      f.write(f"- **Average Core Performance:** {avg_core_time:.0f} ns\n")
                  
                  if mcp_tests:
                      avg_mcp_time = np.mean([p['time_ns'] for p in mcp_tests])
                      f.write(f"- **Average MCP Performance:** {avg_mcp_time:.0f} ns\n")
                  
                  if codeprism_suite_tests:
                      avg_suite_time = np.mean([p['time_ns'] for p in codeprism_suite_tests])
                      f.write(f"- **Average CodePrism Suite Performance:** {avg_suite_time:.0f} ns ({avg_suite_time/1e6:.1f} ms)\n")
                  
                  if codeprism_tool_tests:
                      avg_tool_time = np.mean([p['time_ns'] for p in codeprism_tool_tests])
                      f.write(f"- **Average CodePrism Tool Performance:** {avg_tool_time:.0f} ns ({avg_tool_time/1e6:.1f} ms)\n")
              
              f.write("\n")
              
              # Regression analysis
              if regressions:
                  f.write("## ‚ö†Ô∏è Performance Regressions Detected\n\n")
                  for reg in regressions[:10]:  # Top 10 regressions
                      f.write(f"- **{reg['test']}**: {reg['regression_percent']:.1f}% slower\n")
                  f.write("\n")
              else:
                  f.write("## ‚úÖ No Significant Regressions Detected\n\n")
              
              # Improvements
              if improvements:
                  f.write("## üöÄ Performance Improvements\n\n")
                  for imp in improvements[:10]:  # Top 10 improvements
                      f.write(f"- **{imp['test']}**: {imp['improvement_percent']:.1f}% faster\n")
                  f.write("\n")
              
              # Historical trend summary
              if historical_data:
                  f.write("## üìà Historical Trend Summary\n\n")
                  f.write(f"- **Historical Data Points:** {len(historical_data)}\n")
                  f.write(f"- **Current Data Points:** {len(performance_data)}\n")
                  f.write(f"- **Total Tracked Tests:** {len(set([p['test_name'] for p in all_data]))}\n")
              else:
                  f.write("## üìä Baseline Establishment\n\n")
                  f.write("This is the first performance tracking run. Establishing baseline metrics.\n")
          
          # Save current data for future analysis
          os.makedirs('performance-data', exist_ok=True)
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          with open(f'performance-data/performance_{timestamp}.json', 'w') as f:
              json.dump(performance_data, f, indent=2)
          
          # Output for GitHub Actions
          print(f"REGRESSIONS_COUNT={len(regressions)}")
          print(f"IMPROVEMENTS_COUNT={len(improvements)}")
          print(f"TOTAL_TESTS={len(performance_data)}")
          
          if regressions:
              print("PERFORMANCE_STATUS=regression")
          elif improvements:
              print("PERFORMANCE_STATUS=improvement")
          else:
              print("PERFORMANCE_STATUS=stable")
          PYTHON

      - name: Create performance visualization
        run: |
          python3 - << 'PYTHON'
          import json
          import matplotlib.pyplot as plt
          import pandas as pd
          import numpy as np
          from datetime import datetime
          import glob
          import os
          
          # Load all performance data
          all_data = []
          
          # Current data
          try:
              with open('current-performance.json', 'r') as f:
                  current = json.load(f)
              for result in current.get('results', []):
                  all_data.append({
                      'timestamp': datetime.now(),
                      'test_name': result.get('id', ''),
                      'time_ns': result.get('typical', {}).get('estimate', 0)
                  })
          except:
              pass
          
          # Historical data
          if os.path.exists('performance-history'):
              for history_file in glob.glob('performance-history/*.json'):
                  try:
                      with open(history_file, 'r') as f:
                          historical = json.load(f)
                      for item in historical:
                          all_data.append({
                              'timestamp': pd.to_datetime(item['timestamp']),
                              'test_name': item['test_name'],
                              'time_ns': item['time_ns']
                          })
                  except:
                      pass
          
          if all_data:
              df = pd.DataFrame(all_data)
              
              # Create performance trend charts
              plt.figure(figsize=(15, 10))
              
              # Top 6 most frequently tested functions
              test_counts = df['test_name'].value_counts()
              top_tests = test_counts.head(6).index
              
              for i, test_name in enumerate(top_tests, 1):
                  plt.subplot(2, 3, i)
                  test_data = df[df['test_name'] == test_name].sort_values('timestamp')
                  
                  if len(test_data) > 1:
                      plt.plot(test_data['timestamp'], test_data['time_ns'], 'o-', alpha=0.7)
                      plt.title(f"{test_name[:30]}..." if len(test_name) > 30 else test_name, fontsize=10)
                      plt.ylabel('Time (ns)')
                      plt.xticks(rotation=45)
                      plt.grid(True, alpha=0.3)
              
              plt.tight_layout()
              plt.savefig('performance-trends.png', dpi=150, bbox_inches='tight')
              print("Generated performance trends visualization")
          else:
              print("No data available for visualization")
          PYTHON

      - name: Upload performance data
        uses: actions/upload-artifact@v4
        with:
          name: performance-history
          path: |
            performance-data/
            current-performance.json
            mcp-performance.json
            test-harness-performance.txt
            performance-trend-report.md
            performance-trends.png
            codeprism-benchmarking/
          retention-days: 180

      - name: Comment on performance changes
        if: github.event_name == 'push' && (steps.trend-analysis.outputs.REGRESSIONS_COUNT > 0 || steps.trend-analysis.outputs.IMPROVEMENTS_COUNT > 3)
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read performance report
            let reportContent = '';
            if (fs.existsSync('performance-trend-report.md')) {
              reportContent = fs.readFileSync('performance-trend-report.md', 'utf8');
            }
            
            const regressions = parseInt('${{ steps.trend-analysis.outputs.REGRESSIONS_COUNT }}') || 0;
            const improvements = parseInt('${{ steps.trend-analysis.outputs.IMPROVEMENTS_COUNT }}') || 0;
            
            const title = regressions > 0 ? 
              `‚ö†Ô∏è Performance Regressions Detected (${regressions})` :
              `üöÄ Performance Improvements Detected (${improvements})`;
            
            const body = `# ${title}
            
            Performance tracking has detected significant changes in this commit.
            
            ## Summary
            - **Regressions:** ${regressions}
            - **Improvements:** ${improvements}
            - **Total Tests:** ${{ steps.trend-analysis.outputs.TOTAL_TESTS }}
            
            ${reportContent}
            
            ---
            *Automated by Performance Tracking CI*`;
            
            // Create or update issue for regressions
            if (regressions > 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Performance Regression Alert - ${regressions} regressions detected`,
                body: body,
                labels: ['performance', 'regression', 'priority-high']
              });
            }
            
            // Comment on commit for improvements
            if (improvements > 3) {
              await github.rest.repos.createCommitComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                commit_sha: context.sha,
                body: `## üöÄ Performance Improvements
                
                This commit includes ${improvements} performance improvements!
                
                See the full performance report in the workflow artifacts.`
              });
            }

  # Performance dashboard update
  update-dashboard:
    name: Update Performance Dashboard
    runs-on: ubuntu-latest
    needs: performance-tracking
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download performance data
        uses: actions/download-artifact@v4
        with:
          name: performance-history
          path: performance-data/

      - name: Generate performance dashboard
        run: |
          python3 - << 'PYTHON'
          import json
          import pandas as pd
          import matplotlib.pyplot as plt
          import seaborn as sns
          from datetime import datetime, timedelta
          import glob
          import os
          
          # Load all performance data
          all_data = []
          for data_file in glob.glob('performance-data/*.json'):
              try:
                  with open(data_file, 'r') as f:
                      data = json.load(f)
                  all_data.extend(data)
              except:
                  pass
          
          if not all_data:
              print("No performance data available")
              exit(0)
          
          df = pd.DataFrame(all_data)
          df['timestamp'] = pd.to_datetime(df['timestamp'])
          
          # Generate comprehensive dashboard
          plt.style.use('seaborn-v0_8')
          fig, axes = plt.subplots(2, 2, figsize=(20, 15))
          fig.suptitle('MCP Test Harness Performance Dashboard', fontsize=16, fontweight='bold')
          
          # 1. Performance over time (top tests)
          ax1 = axes[0, 0]
          top_tests = df['test_name'].value_counts().head(5).index
          for test in top_tests:
              test_data = df[df['test_name'] == test].sort_values('timestamp')
              ax1.plot(test_data['timestamp'], test_data['time_ns'], 'o-', 
                      label=test[:20] + '...' if len(test) > 20 else test, alpha=0.7)
          ax1.set_title('Performance Trends - Top 5 Tests')
          ax1.set_ylabel('Execution Time (ns)')
          ax1.legend(fontsize=8)
          ax1.grid(True, alpha=0.3)
          
          # 2. Performance distribution
          ax2 = axes[0, 1]
          recent_data = df[df['timestamp'] > (datetime.now() - timedelta(days=7))]
          if not recent_data.empty:
              ax2.hist(recent_data['time_ns'], bins=30, alpha=0.7, edgecolor='black')
              ax2.set_title('Performance Distribution (Last 7 Days)')
              ax2.set_xlabel('Execution Time (ns)')
              ax2.set_ylabel('Frequency')
          
          # 3. Category comparison
          ax3 = axes[1, 0]
          if 'category' in df.columns:
              category_perf = df.groupby('category')['time_ns'].mean()
              ax3.bar(category_perf.index, category_perf.values, alpha=0.7)
              ax3.set_title('Average Performance by Category')
              ax3.set_ylabel('Average Time (ns)')
          else:
              ax3.text(0.5, 0.5, 'Category data not available', 
                      ha='center', va='center', transform=ax3.transAxes)
          
          # 4. Performance heatmap (last 30 days)
          ax4 = axes[1, 1]
          recent_month = df[df['timestamp'] > (datetime.now() - timedelta(days=30))]
          if not recent_month.empty and len(recent_month) > 10:
              # Create pivot for heatmap
              recent_month['date'] = recent_month['timestamp'].dt.date
              pivot_data = recent_month.groupby(['date', 'test_name'])['time_ns'].mean().unstack()
              
              if not pivot_data.empty:
                  sns.heatmap(pivot_data.T, ax=ax4, cmap='YlOrRd', cbar_kws={'label': 'Time (ns)'})
                  ax4.set_title('Performance Heatmap (Last 30 Days)')
                  ax4.set_xlabel('Date')
                  ax4.set_ylabel('Test Name')
              else:
                  ax4.text(0.5, 0.5, 'Insufficient data for heatmap', 
                          ha='center', va='center', transform=ax4.transAxes)
          else:
              ax4.text(0.5, 0.5, 'Insufficient recent data', 
                      ha='center', va='center', transform=ax4.transAxes)
          
          plt.tight_layout()
          plt.savefig('performance-dashboard.png', dpi=150, bbox_inches='tight')
          
          # Generate summary statistics
          summary_stats = {
              'total_data_points': len(df),
              'unique_tests': df['test_name'].nunique(),
              'date_range': {
                  'start': df['timestamp'].min().isoformat(),
                  'end': df['timestamp'].max().isoformat()
              },
              'average_performance': df['time_ns'].mean(),
              'performance_std': df['time_ns'].std()
          }
          
          with open('dashboard-summary.json', 'w') as f:
              json.dump(summary_stats, f, indent=2)
          
          print("Generated performance dashboard successfully")
          PYTHON

      - name: Upload dashboard
        uses: actions/upload-artifact@v4
        with:
          name: performance-dashboard
          path: |
            performance-dashboard.png
            dashboard-summary.json
          retention-days: 90
