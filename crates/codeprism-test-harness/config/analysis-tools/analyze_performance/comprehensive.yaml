# Analyze Performance Tool - Comprehensive Test Configuration
# Tests the analyze_performance MCP tool for identifying performance issues and optimization opportunities

global:
  max_global_concurrency: 2
  timeout_seconds: 45
  fail_fast: false
  default_project_path: "test-projects/python-sample"

# Performance monitoring for performance analysis
performance:
  enable_monitoring: true
  baseline_storage_path: "baselines/analysis-tools/"
  regression_detection:
    warning_threshold_percent: 40.0
    error_threshold_percent: 80.0

test_suites:
  - name: "Performance Analysis - Core Detection"
    description: "Test performance issue detection and optimization suggestions"
    test_cases:
      # Basic performance analysis
      - id: "python_performance_analysis"
        tool_name: "analyze_performance"
        description: "Comprehensive performance analysis for Python project"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          analysis_types: ["algorithmic", "memory", "io", "database"]
          detect_bottlenecks: true
          suggest_optimizations: true
          include_complexity_analysis: true
          performance_patterns: ["n_squared", "inefficient_loops", "memory_leaks", "unnecessary_io"]
          confidence_threshold: 0.6
        expected:
          patterns:
            - key: "result.performance_issues"
              validation:
                type: "array"
              required: true
            - key: "result.optimization_suggestions"
              validation:
                type: "array"
              required: true
            - key: "result.bottleneck_analysis"
              validation:
                type: "exists"
              required: true
            - key: "result.performance_score"
              validation:
                type: "range"
                min: 0.0
                max: 100.0
              required: true
          performance_requirements:
            max_execution_time_ms: 10000
            max_memory_usage_mb: 128
          custom_scripts:
            - name: "validate_performance_analysis"
              language: "python"
              script: |
                def validate(response):
                    result = response.get('result', {})
                    performance_issues = result.get('performance_issues', [])
                    optimizations = result.get('optimization_suggestions', [])
                    bottlenecks = result.get('bottleneck_analysis', {})
                    
                    # Validate performance issues structure
                    issues_score = 0.8  # Default good score if no issues
                    if performance_issues:
                        valid_issues = 0
                        severity_distribution = {'high': 0, 'medium': 0, 'low': 0}
                        
                        for issue in performance_issues:
                            required_fields = ['type', 'severity', 'location', 'description', 'confidence']
                            if all(field in issue for field in required_fields):
                                valid_issues += 1
                                
                                # Track severity distribution
                                severity = issue.get('severity', 'low').lower()
                                if severity in severity_distribution:
                                    severity_distribution[severity] += 1
                                
                                # Bonus for high confidence issues
                                confidence = issue.get('confidence', 0.0)
                                if confidence >= 0.8:
                                    valid_issues += 0.2
                        
                        issues_score = valid_issues / len(performance_issues)
                    
                    # Validate optimization suggestions quality
                    optimization_score = 0.8  # Default if no optimizations needed
                    if optimizations:
                        quality_suggestions = 0
                        for suggestion in optimizations:
                            required_fields = ['optimization_type', 'expected_improvement', 'implementation_effort']
                            if all(field in suggestion for field in required_fields):
                                quality_suggestions += 1
                                
                                # Bonus for quantified improvements
                                improvement = suggestion.get('expected_improvement', {})
                                if isinstance(improvement, dict) and 'percentage' in improvement:
                                    quality_suggestions += 0.3
                        
                        optimization_score = quality_suggestions / len(optimizations)
                    
                    # Validate bottleneck analysis
                    bottleneck_score = 0.0
                    if bottlenecks:
                        required_bottleneck_fields = ['critical_paths', 'resource_usage', 'timing_analysis']
                        bottleneck_score = sum(1 for field in required_bottleneck_fields if field in bottlenecks) / len(required_bottleneck_fields)
                    
                    overall_score = (issues_score * 0.4) + (optimization_score * 0.4) + (bottleneck_score * 0.2)
                    
                    return True, overall_score, f"Performance analysis: {len(performance_issues)} issues, {len(optimizations)} optimizations"

      # Algorithm complexity detection
      - id: "algorithm_complexity_detection"
        tool_name: "analyze_performance"
        description: "Detect inefficient algorithms and suggest improvements"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          focus_areas: ["algorithmic_complexity", "data_structure_efficiency"]
          detect_patterns: ["nested_loops", "redundant_computations", "inefficient_data_structures"]
          analyze_time_complexity: true
          analyze_space_complexity: true
          suggest_alternatives: true
        expected:
          patterns:
            - key: "result.complexity_issues"
              validation:
                type: "array"
              required: true
            - key: "result.algorithm_suggestions"
              validation:
                type: "array"
              required: false
            - key: "result.complexity_analysis"
              validation:
                type: "exists"
              required: true
          custom_scripts:
            - name: "validate_algorithm_analysis"
              language: "python"
              script: |
                def validate(response):
                    result = response.get('result', {})
                    complexity_issues = result.get('complexity_issues', [])
                    algorithm_suggestions = result.get('algorithm_suggestions', [])
                    complexity_analysis = result.get('complexity_analysis', {})
                    
                    # Validate complexity issues
                    complexity_score = 0.8  # Default good score
                    if complexity_issues:
                        valid_complexity_issues = 0
                        for issue in complexity_issues:
                            required_fields = ['complexity_type', 'current_complexity', 'impact_score']
                            if all(field in issue for field in required_fields):
                                valid_complexity_issues += 1
                                
                                # Bonus for Big O notation
                                if 'big_o_notation' in issue:
                                    valid_complexity_issues += 0.2
                        
                        complexity_score = valid_complexity_issues / len(complexity_issues)
                    
                    # Validate algorithm suggestions
                    suggestion_score = 0.8  # Default good score
                    if algorithm_suggestions:
                        quality_algo_suggestions = 0
                        for suggestion in algorithm_suggestions:
                            required_fields = ['alternative_algorithm', 'complexity_improvement', 'trade_offs']
                            if all(field in suggestion for field in required_fields):
                                quality_algo_suggestions += 1
                        
                        suggestion_score = quality_algo_suggestions / len(algorithm_suggestions)
                    
                    # Validate complexity analysis structure
                    analysis_score = 0.0
                    if complexity_analysis:
                        expected_analysis_fields = ['time_complexity_summary', 'space_complexity_summary']
                        analysis_score = sum(1 for field in expected_analysis_fields if field in complexity_analysis) / len(expected_analysis_fields)
                    
                    overall_score = (complexity_score * 0.5) + (suggestion_score * 0.3) + (analysis_score * 0.2)
                    
                    return True, overall_score, f"Algorithm analysis: {len(complexity_issues)} complexity issues, {len(algorithm_suggestions)} suggestions"

      # Multi-language performance comparison
      - id: "multi_language_performance_analysis"
        tool_name: "analyze_performance"
        description: "Compare performance patterns across different programming languages"
        enabled: true
        project_path: "test-projects"
        input_params:
          languages: ["python", "javascript", "rust", "java"]
          comparative_analysis: true
          language_specific_patterns: true
          cross_language_benchmarks: true
          idiomatic_performance_checks: true
        expected:
          patterns:
            - key: "result.language_performance_comparison"
              validation:
                type: "exists"
              required: true
            - key: "result.language_specific_issues"
              validation:
                type: "exists"
              required: true
          custom_scripts:
            - name: "validate_multi_language_analysis"
              language: "python"
              script: |
                def validate(response):
                    result = response.get('result', {})
                    lang_comparison = result.get('language_performance_comparison', {})
                    lang_issues = result.get('language_specific_issues', {})
                    
                    # Validate language comparison
                    comparison_score = 0.0
                    expected_languages = ['python', 'javascript', 'rust', 'java']
                    
                    if lang_comparison:
                        analyzed_languages = 0
                        for lang in expected_languages:
                            if lang in lang_comparison:
                                lang_data = lang_comparison[lang]
                                required_fields = ['performance_score', 'common_issues', 'optimization_potential']
                                if all(field in lang_data for field in required_fields):
                                    analyzed_languages += 1
                        
                        comparison_score = analyzed_languages / len(expected_languages)
                    
                    # Validate language-specific issues
                    issues_score = 0.0
                    if lang_issues:
                        valid_issue_categories = 0
                        total_categories = 0
                        
                        for lang, issues in lang_issues.items():
                            total_categories += 1
                            if isinstance(issues, list) and len(issues) >= 0:
                                valid_issue_categories += 1
                                
                                # Bonus for detailed issue descriptions
                                for issue in issues:
                                    if isinstance(issue, dict) and 'pattern' in issue and 'recommendation' in issue:
                                        valid_issue_categories += 0.1
                        
                        if total_categories > 0:
                            issues_score = valid_issue_categories / total_categories
                    
                    overall_score = (comparison_score * 0.7) + (issues_score * 0.3)
                    
                    return True, overall_score, f"Multi-language: {len(lang_comparison)} languages compared, {len(lang_issues)} issue categories"

  - name: "Performance Analysis - Advanced Detection"
    description: "Test advanced performance issue detection patterns"
    test_cases:
      # Memory leak detection
      - id: "memory_leak_detection"
        tool_name: "analyze_performance"
        description: "Detect potential memory leaks and resource management issues"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          focus_areas: ["memory_management", "resource_leaks"]
          detect_patterns: ["unclosed_resources", "circular_references", "memory_accumulation"]
          analyze_gc_pressure: true
          suggest_memory_optimizations: true
        expected:
          patterns:
            - key: "result.memory_issues"
              validation:
                type: "array"
              required: true
            - key: "result.resource_management_analysis"
              validation:
                type: "exists"
              required: true

      # Database performance analysis
      - id: "database_performance_analysis"
        tool_name: "analyze_performance"
        description: "Analyze database-related performance issues"
        enabled: true
        project_path: "test-projects/dependency-test-project"
        input_params:
          focus_areas: ["database_queries", "orm_efficiency"]
          detect_patterns: ["n_plus_one", "missing_indexes", "inefficient_joins", "large_result_sets"]
          analyze_query_patterns: true
          suggest_db_optimizations: true
        expected:
          patterns:
            - key: "result.database_issues"
              validation:
                type: "array"
              required: true
            - key: "result.query_optimization_suggestions"
              validation:
                type: "array"
              required: false

      # Performance regression detection
      - id: "performance_regression_detection"
        tool_name: "analyze_performance"
        description: "Detect potential performance regressions in code changes"
        enabled: true
        project_path: "test-projects/rust-test-project"
        input_params:
          regression_analysis: true
          baseline_comparison: true
          change_impact_analysis: true
          performance_trend_analysis: true
        expected:
          patterns:
            - key: "result.regression_risks"
              validation:
                type: "array"
              required: false
            - key: "result.trend_analysis"
              validation:
                type: "exists"
              required: false

# Performance baselines for performance analysis tool
baselines:
  python_performance_analysis:
    average_execution_time_ms: 8000.0
    peak_memory_mb: 112.0
    throughput_ops_per_sec: 0.125
  algorithm_complexity_detection:
    average_execution_time_ms: 6000.0
    peak_memory_mb: 96.0
    throughput_ops_per_sec: 0.167
  multi_language_performance_analysis:
    average_execution_time_ms: 12000.0
    peak_memory_mb: 160.0
    throughput_ops_per_sec: 0.083
  memory_leak_detection:
    average_execution_time_ms: 9000.0
    peak_memory_mb: 128.0
    throughput_ops_per_sec: 0.111
  database_performance_analysis:
    average_execution_time_ms: 7000.0
    peak_memory_mb: 104.0
    throughput_ops_per_sec: 0.143
