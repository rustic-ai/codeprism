# Concurrent Access Edge Case Testing
# Tests server stability and request isolation under concurrent load

global:
  max_global_concurrency: 8  # High concurrency for stress testing
  timeout_seconds: 90
  fail_fast: false
  default_project_path: "test-projects/python-sample"

# Performance monitoring for concurrent access testing
performance:
  enable_monitoring: true
  baseline_storage_path: "baselines/edge-cases/"
  regression_detection:
    warning_threshold_percent: 80.0
    error_threshold_percent: 150.0

test_suites:
  - name: "Concurrent Access - Request Isolation"
    description: "Test request isolation and data consistency under concurrent load"
    test_cases:
      # Concurrent repository analysis
      - id: "concurrent_repository_stats_isolation"
        tool_name: "repository_stats"
        description: "Concurrent repository stats with different parameters"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          include_complexity: true
          include_file_types: true
        expected:
          patterns:
            - key: "result"
              validation:
                type: "exists"
              required: true
            - key: "result.total_files"
              validation:
                type: "range"
                min: 1
                max: 100
              required: true
          performance_requirements:
            max_execution_time_ms: 10000
            max_memory_usage_mb: 64
        # Run multiple concurrent instances
        concurrency_config:
          concurrent_instances: 4
          instance_variations:
            - input_params:
                include_complexity: true
                include_file_types: false
            - input_params:
                include_complexity: false
                include_file_types: true
            - input_params:
                include_complexity: true
                include_file_types: true
                max_depth: 5
            - input_params:
                include_complexity: false
                include_file_types: false
          validate_consistency: true
          custom_scripts:
            - name: "validate_concurrent_isolation"
              language: "python"
              script: |
                def validate(responses):
                    if len(responses) != 4:
                        return False, 0.0, f"Expected 4 responses, got {len(responses)}"
                    
                    # Check each response is valid
                    valid_responses = 0
                    for i, response in enumerate(responses):
                        if 'result' in response:
                            result = response['result']
                            if 'total_files' in result and result['total_files'] > 0:
                                valid_responses += 1
                    
                    isolation_score = valid_responses / len(responses)
                    
                    if isolation_score == 1.0:
                        return True, 1.0, "Perfect request isolation - all requests successful"
                    elif isolation_score >= 0.75:
                        return True, 0.8, f"Good isolation: {valid_responses}/{len(responses)} successful"
                    else:
                        return False, 0.4, f"Poor isolation: {valid_responses}/{len(responses)} successful"

      # Concurrent symbol search
      - id: "concurrent_search_symbols_different_queries"
        tool_name: "search_symbols"
        description: "Concurrent symbol searches with different queries"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          query: "class:*"
          language: "python"
        expected:
          patterns:
            - key: "result"
              validation:
                type: "exists"
              required: true
        concurrency_config:
          concurrent_instances: 3
          instance_variations:
            - input_params:
                query: "class:*"
                language: "python"
            - input_params:
                query: "function:*"
                language: "python"
            - input_params:
                query: "variable:*"
                language: "python"
          validate_consistency: true

      # Concurrent security analysis
      - id: "concurrent_analyze_security_isolation"
        tool_name: "analyze_security"
        description: "Concurrent security analysis with isolation validation"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          scan_type: "basic"
          include_dependencies: false
        expected:
          patterns:
            - key: "result"
              validation:
                type: "exists"
              required: true
        concurrency_config:
          concurrent_instances: 3
          instance_variations:
            - input_params:
                scan_type: "basic"
                severity_filter: "high"
            - input_params:
                scan_type: "basic"
                severity_filter: "medium"
            - input_params:
                scan_type: "basic"
                include_dependencies: false

  - name: "Concurrent Access - Resource Contention"
    description: "Test resource contention and fair allocation"
    test_cases:
      # Mixed concurrent operations
      - id: "concurrent_mixed_operations"
        tool_name: "mixed_operations"
        description: "Mixed concurrent operations testing"
        enabled: true
        project_path: "test-projects/dependency-test-project"
        expected:
          patterns:
            - key: "concurrent_results"
              validation:
                type: "array"
              required: true
        # Custom test runner for mixed operations
        custom_test_runner:
          name: "run_mixed_concurrent_operations"
          language: "python"
          script: |
            import asyncio
            import concurrent.futures
            import time
            
            async def run_concurrent_test():
                # Define different operations to run concurrently
                operations = [
                    ("repository_stats", {"include_complexity": True}),
                    ("search_symbols", {"query": "class:*", "language": "python"}),
                    ("analyze_security", {"scan_type": "basic"}),
                    ("find_dependencies", {"target_symbol": "User", "language": "python"}),
                    ("analyze_performance", {"analysis_type": "basic"}),
                ]
                
                start_time = time.time()
                results = []
                
                # Run operations concurrently
                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                    futures = []
                    for tool_name, params in operations:
                        future = executor.submit(call_mcp_tool, tool_name, params)
                        futures.append((tool_name, future))
                    
                    for tool_name, future in futures:
                        try:
                            result = future.result(timeout=30)
                            results.append({
                                "tool": tool_name,
                                "success": "result" in result,
                                "response": result
                            })
                        except Exception as e:
                            results.append({
                                "tool": tool_name,
                                "success": False,
                                "error": str(e)
                            })
                
                end_time = time.time()
                execution_time = end_time - start_time
                
                # Validate results
                successful_ops = sum(1 for r in results if r["success"])
                success_rate = successful_ops / len(operations)
                
                return {
                    "concurrent_results": results,
                    "success_rate": success_rate,
                    "execution_time": execution_time,
                    "resource_contention_handled": success_rate >= 0.8
                }

      # High-frequency requests
      - id: "concurrent_high_frequency_requests"
        tool_name: "repository_stats"
        description: "High-frequency concurrent requests"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          include_complexity: false  # Lighter operation
        expected:
          patterns:
            - key: "result"
              validation:
                type: "exists"
              required: true
        concurrency_config:
          concurrent_instances: 10  # High frequency
          instance_variations: null  # All identical requests
          stagger_requests: true
          stagger_delay_ms: 100
          custom_scripts:
            - name: "validate_high_frequency_handling"
              language: "python"
              script: |
                def validate(responses):
                    successful = sum(1 for r in responses if 'result' in r)
                    success_rate = successful / len(responses)
                    
                    if success_rate >= 0.9:
                        return True, 1.0, f"Excellent high-frequency handling: {successful}/{len(responses)}"
                    elif success_rate >= 0.7:
                        return True, 0.8, f"Good high-frequency handling: {successful}/{len(responses)}"
                    else:
                        return False, 0.4, f"Poor high-frequency handling: {successful}/{len(responses)}"

  - name: "Concurrent Access - Server Stability"
    description: "Test server stability under sustained concurrent load"
    test_cases:
      # Sustained load test
      - id: "concurrent_sustained_load"
        tool_name: "search_content"
        description: "Sustained concurrent load testing"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          query: "def "
          include_code: true
        expected:
          patterns:
            - key: "result"
              validation:
                type: "exists"
              required: true
        # Sustained load configuration
        load_test_config:
          duration_seconds: 30
          requests_per_second: 2
          concurrent_connections: 4
          ramp_up_seconds: 5
          custom_scripts:
            - name: "validate_sustained_load"
              language: "python"
              script: |
                def validate(load_test_results):
                    total_requests = load_test_results.get('total_requests', 0)
                    successful_requests = load_test_results.get('successful_requests', 0)
                    failed_requests = load_test_results.get('failed_requests', 0)
                    avg_response_time = load_test_results.get('avg_response_time_ms', 0)
                    
                    success_rate = successful_requests / total_requests if total_requests > 0 else 0
                    
                    # Evaluate server stability
                    if success_rate >= 0.95 and avg_response_time < 5000:
                        return True, 1.0, f"Excellent stability: {success_rate:.2%} success, {avg_response_time:.0f}ms avg"
                    elif success_rate >= 0.85 and avg_response_time < 10000:
                        return True, 0.8, f"Good stability: {success_rate:.2%} success, {avg_response_time:.0f}ms avg"
                    elif success_rate >= 0.7:
                        return True, 0.6, f"Acceptable stability: {success_rate:.2%} success, {avg_response_time:.0f}ms avg"
                    else:
                        return False, 0.3, f"Poor stability: {success_rate:.2%} success, {avg_response_time:.0f}ms avg"

      # Memory leak detection under concurrent load
      - id: "concurrent_memory_leak_detection"
        tool_name: "analyze_complexity"
        description: "Memory leak detection under concurrent load"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          analysis_scope: "project"
        expected:
          patterns:
            - key: "result"
              validation:
                type: "exists"
              required: true
        memory_leak_test:
          iterations: 20
          concurrent_requests: 3
          memory_growth_threshold_mb: 50
          custom_scripts:
            - name: "validate_memory_stability"
              language: "python"
              script: |
                def validate(memory_test_results):
                    initial_memory = memory_test_results.get('initial_memory_mb', 0)
                    final_memory = memory_test_results.get('final_memory_mb', 0)
                    peak_memory = memory_test_results.get('peak_memory_mb', 0)
                    memory_growth = final_memory - initial_memory
                    
                    if memory_growth < 20:
                        return True, 1.0, f"No memory leak detected: {memory_growth:.1f}MB growth"
                    elif memory_growth < 50:
                        return True, 0.7, f"Minimal memory growth: {memory_growth:.1f}MB"
                    else:
                        return False, 0.3, f"Potential memory leak: {memory_growth:.1f}MB growth"

# Performance baselines for concurrent access testing
baselines:
  concurrent_repository_stats_isolation:
    average_execution_time_ms: 3000.0
    peak_memory_mb: 48.0
    throughput_ops_per_sec: 1.33
  concurrent_search_symbols_different_queries:
    average_execution_time_ms: 2500.0
    peak_memory_mb: 40.0
    throughput_ops_per_sec: 1.2
  concurrent_analyze_security_isolation:
    average_execution_time_ms: 6000.0
    peak_memory_mb: 64.0
    throughput_ops_per_sec: 0.5
  concurrent_mixed_operations:
    average_execution_time_ms: 8000.0
    peak_memory_mb: 80.0
    throughput_ops_per_sec: 0.625
