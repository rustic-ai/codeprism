# Content Stats Tool - Comprehensive Test Configuration  
# Tests the content_stats MCP tool for generating content and structure statistics

global:
  max_global_concurrency: 3
  timeout_seconds: 25
  fail_fast: false
  default_project_path: "test-projects"

# Performance monitoring for content statistics
performance:
  enable_monitoring: true
  baseline_storage_path: "baselines/search-tools/"
  regression_detection:
    warning_threshold_percent: 30.0
    error_threshold_percent: 70.0

test_suites:
  - name: "Content Statistics - Core Analysis"
    description: "Test content and structure statistics generation"
    test_cases:
      # Basic project statistics
      - id: "basic_project_stats"
        tool_name: "content_stats"
        description: "Generate basic statistics for multi-language project"
        enabled: true
        project_path: "test-projects"
        input_params:
          include_languages: ["python", "javascript", "rust", "java"]
          include_file_counts: true
          include_line_counts: true
          include_size_statistics: true
          exclude_patterns: ["**/node_modules/**", "**/__pycache__/**", "**/target/**"]
        expected:
          patterns:
            - key: "result.project_summary"
              validation:
                type: "exists"
              required: true
            - key: "result.language_statistics"
              validation:
                type: "exists"
              required: true
            - key: "result.total_files"
              validation:
                type: "range"
                min: 10
                max: 500
              required: true
            - key: "result.total_lines"
              validation:
                type: "range"
                min: 100
                max: 100000
              required: true
          performance_requirements:
            max_execution_time_ms: 6000
            max_memory_usage_mb: 96
          custom_scripts:
            - name: "validate_project_statistics"
              language: "python"
              script: |
                def validate(response):
                    result = response.get('result', {})
                    project_summary = result.get('project_summary', {})
                    language_stats = result.get('language_statistics', {})
                    
                    # Validate project summary structure
                    required_summary_fields = ['total_files', 'total_lines', 'total_size_bytes']
                    summary_score = 0.0
                    
                    for field in required_summary_fields:
                        if field in project_summary:
                            value = project_summary[field]
                            if isinstance(value, (int, float)) and value > 0:
                                summary_score += 1.0 / len(required_summary_fields)
                    
                    # Validate language statistics
                    language_score = 0.0
                    if language_stats:
                        expected_languages = ['python', 'javascript', 'rust', 'java']
                        valid_languages = 0
                        
                        for lang in expected_languages:
                            if lang in language_stats:
                                lang_data = language_stats[lang]
                                required_lang_fields = ['file_count', 'line_count', 'percentage']
                                if all(field in lang_data for field in required_lang_fields):
                                    valid_languages += 1
                        
                        language_score = valid_languages / len(expected_languages)
                    
                    overall_score = (summary_score * 0.6) + (language_score * 0.4)
                    
                    return True, overall_score, f"Project stats: {len(language_stats)} languages, summary quality: {summary_score:.2f}"

      # Detailed code analysis statistics
      - id: "detailed_code_analysis_stats"
        tool_name: "content_stats"
        description: "Generate detailed code analysis statistics including complexity metrics"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          analysis_depth: "detailed"
          include_complexity_stats: true
          include_comment_ratio: true
          include_function_statistics: true
          include_class_statistics: true
          include_import_analysis: true
          calculate_technical_debt: true
        expected:
          patterns:
            - key: "result.code_quality_metrics"
              validation:
                type: "exists"
              required: true
            - key: "result.comment_statistics"
              validation:
                type: "exists"
              required: true
            - key: "result.function_statistics"
              validation:
                type: "exists"
              required: true
          custom_scripts:
            - name: "validate_detailed_analysis"
              language: "python"
              script: |
                def validate(response):
                    result = response.get('result', {})
                    quality_metrics = result.get('code_quality_metrics', {})
                    comment_stats = result.get('comment_statistics', {})
                    function_stats = result.get('function_statistics', {})
                    
                    # Validate code quality metrics
                    quality_score = 0.0
                    if quality_metrics:
                        expected_metrics = ['average_complexity', 'technical_debt_score', 'maintainability_index']
                        for metric in expected_metrics:
                            if metric in quality_metrics:
                                quality_score += 1.0 / len(expected_metrics)
                    
                    # Validate comment statistics
                    comment_score = 0.0
                    if comment_stats:
                        expected_comment_fields = ['comment_ratio', 'commented_functions', 'documentation_coverage']
                        for field in expected_comment_fields:
                            if field in comment_stats:
                                comment_score += 1.0 / len(expected_comment_fields)
                    
                    # Validate function statistics
                    function_score = 0.0
                    if function_stats:
                        expected_func_fields = ['total_functions', 'average_length', 'complexity_distribution']
                        for field in expected_func_fields:
                            if field in function_stats:
                                function_score += 1.0 / len(expected_func_fields)
                    
                    overall_score = (quality_score * 0.4) + (comment_score * 0.3) + (function_score * 0.3)
                    
                    return True, overall_score, f"Detailed analysis: quality={quality_score:.2f}, comments={comment_score:.2f}, functions={function_score:.2f}"

      # Cross-project comparison statistics
      - id: "cross_project_comparison"
        tool_name: "content_stats"
        description: "Generate comparative statistics across multiple projects"
        enabled: true
        project_path: "test-projects"
        input_params:
          analysis_scope: "multi_project"
          compare_languages: true
          compare_complexity: true
          compare_structure_patterns: true
          generate_benchmarks: true
          include_best_practices_analysis: true
        expected:
          patterns:
            - key: "result.project_comparisons"
              validation:
                type: "exists"
              required: true
            - key: "result.language_comparisons"
              validation:
                type: "exists"
              required: true
            - key: "result.benchmark_data"
              validation:
                type: "exists"
              required: false
          custom_scripts:
            - name: "validate_cross_project_analysis"
              language: "python"
              script: |
                def validate(response):
                    result = response.get('result', {})
                    project_comparisons = result.get('project_comparisons', {})
                    language_comparisons = result.get('language_comparisons', {})
                    
                    # Validate project comparison structure
                    comparison_score = 0.0
                    if project_comparisons:
                        valid_comparisons = 0
                        for proj_name, comparison in project_comparisons.items():
                            required_fields = ['relative_size', 'complexity_ranking', 'quality_score']
                            if all(field in comparison for field in required_fields):
                                valid_comparisons += 1
                        
                        if project_comparisons:
                            comparison_score = valid_comparisons / len(project_comparisons)
                    
                    # Validate language comparison data
                    language_score = 0.0
                    if language_comparisons:
                        expected_languages = ['python', 'javascript', 'rust', 'java']
                        analyzed_languages = 0
                        
                        for lang in expected_languages:
                            if lang in language_comparisons:
                                lang_comparison = language_comparisons[lang]
                                if 'average_metrics' in lang_comparison and 'project_distribution' in lang_comparison:
                                    analyzed_languages += 1
                        
                        language_score = analyzed_languages / len(expected_languages)
                    
                    overall_score = (comparison_score * 0.6) + (language_score * 0.4)
                    
                    return True, overall_score, f"Cross-project: {len(project_comparisons)} projects, {len(language_comparisons)} languages"

  - name: "Content Statistics - Performance & Scalability"
    description: "Test performance and scalability of content statistics generation"
    test_cases:
      # Large codebase performance test
      - id: "large_codebase_stats_performance"
        tool_name: "content_stats"
        description: "Performance test on large multi-project codebase"
        enabled: true
        project_path: "test-projects"
        input_params:
          analysis_depth: "comprehensive"
          parallel_processing: true
          memory_efficient_mode: true
          progress_reporting: true
          cache_intermediate_results: true
        expected:
          patterns:
            - key: "result.performance_metrics"
              validation:
                type: "exists"
              required: true
            - key: "result.processing_summary"
              validation:
                type: "exists"
              required: true
          performance_requirements:
            max_execution_time_ms: 12000
            max_memory_usage_mb: 180

      # Incremental statistics updates
      - id: "incremental_stats_updates"
        tool_name: "content_stats"
        description: "Test incremental statistics updates for changed files"
        enabled: true
        project_path: "test-projects/python-sample"
        input_params:
          incremental_mode: true
          cache_previous_results: true
          detect_changes: true
          update_affected_metrics: true
        expected:
          patterns:
            - key: "result.incremental_update_summary"
              validation:
                type: "exists"
              required: true
            - key: "result.changed_metrics"
              validation:
                type: "exists"
              required: false

      # Error handling for corrupted files
      - id: "corrupted_files_handling"
        tool_name: "content_stats"
        description: "Handle corrupted or binary files gracefully"
        enabled: true
        project_path: "test-projects"
        input_params:
          include_binary_files: false
          skip_corrupted_files: true
          error_recovery_mode: true
        expected:
          patterns:
            - key: "result.processing_errors"
              validation:
                type: "exists"
              required: false
            - key: "result.skipped_files"
              validation:
                type: "exists"
              required: false
          allow_partial_failure: true

# Performance baselines for content statistics
baselines:
  basic_project_stats:
    average_execution_time_ms: 4000.0
    peak_memory_mb: 80.0
    throughput_ops_per_sec: 0.25
  detailed_code_analysis_stats:
    average_execution_time_ms: 6000.0
    peak_memory_mb: 96.0
    throughput_ops_per_sec: 0.167
  cross_project_comparison:
    average_execution_time_ms: 8000.0
    peak_memory_mb: 128.0
    throughput_ops_per_sec: 0.125
  large_codebase_stats_performance:
    average_execution_time_ms: 10000.0
    peak_memory_mb: 160.0
    throughput_ops_per_sec: 0.1
